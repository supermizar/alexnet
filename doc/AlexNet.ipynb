{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection question: AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation and optimization of a convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a classic convolutional neural network: AlexNet. **Improve its efficiency as much as possible.** Your work will be evaluated by four aspects: *performance, algorithm design, software architecture and readability.*\n",
    "\n",
    "- Neural network should be implemented by hand, open source arch(Caffe, Tensorflow, Theano, Torch) should not be used directly.\n",
    "- Both inference and train algorithm should be constructed, performance should be well considered either.\n",
    "- Project deadline: 2016-12-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve a easy start, I copied some useful code from [[1]](https://www.zybuluo.com/hanbingtao/note/476663) and [[2]](https://www.zybuluo.com/hanbingtao/note/485480), including:\n",
    "- class Layer, class Node, class ConstNode, class Connection in fc_layer_hbt.py\n",
    "- class ConvLayer in conv_layer.py\n",
    "\n",
    "With the codes above, now we have the ability to:\n",
    "- Construct a full connection layer instance with class Layer, forward calculation function not included in those classes\n",
    "- Initialize a convolutional layer with class ConvLayer, including backpropagation algorithm and forward computation method\n",
    "\n",
    "To achieve the goal mentioned at beginning, there exists several works to be finished:\n",
    "- Design a network class, composing convolutional layer and full connection layer together. Both train and predict function should be implemented. \n",
    "- Optimize the training and predicting procedure, improve performance of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inversion of Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we design the network class, it might be wise to take a look at **alexnet.py** in tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = slim.conv2d(inputs, 64, [11, 11], 4, padding='VALID',scope='conv1')\n",
    "net = slim.max_pool2d(net, [3, 3], 2, scope='pool1')\n",
    "net = slim.conv2d(net, 192, [5, 5], scope='conv2')\n",
    "net = slim.max_pool2d(net, [3, 3], 2, scope='pool2')\n",
    "net = slim.conv2d(net, 384, [3, 3], scope='conv3')\n",
    "net = slim.conv2d(net, 384, [3, 3], scope='conv4')\n",
    "net = slim.conv2d(net, 256, [3, 3], scope='conv5')\n",
    "net = slim.max_pool2d(net, [3, 3], 2, scope='pool5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, to make the procedure of network building looks more friendly, principle IoC was applied: instance net is passed to the constructor function of each layer-- my network class will learn from this to take more readability for the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def append_layer(self, layer):\n",
    "        self.layers.append(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Member variable **layers** will be initialized to hold layers in the network. Constructor method of layer will invoke the method *append_layer* to append themselves to the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we all know, the training task of a neural network can be divided into three subtasks:\n",
    "- Forward calculation, or model prediction: given input sample, calculate the output vector due to NN model\n",
    "- Backpropagation-1: $\\delta$ calculation of each layer. Acturally, $\\delta_{l}$ is the partial derivative of the loss function $E_{d}$ on the weighted input vector $net_{l}$: $$\\delta_{l}=\\frac{\\partial E_{d}}{\\partial net_{l}}$$\n",
    "- Backpropagation-2: calculate gradient matrix $\\nabla$ for each layer, then update weight matrix $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_one_sample(self, label, sample, rate):\n",
    "        \"\"\"\n",
    "        train network with one sample\n",
    "        \"\"\"\n",
    "        self.predict(sample)\n",
    "        self.calc_delta(label)\n",
    "        self.update_weight(rate)\n",
    "\n",
    "    def calc_delta(self, label):\n",
    "        \"\"\"\n",
    "        calc delta of each layer\n",
    "        \"\"\"\n",
    "        output_layer = self.layers[-1]\n",
    "        output_layer.calc_output_layer_delta(label)\n",
    "        downstream_layer = output_layer\n",
    "        for layer in self.layers[-2::-1]:\n",
    "            layer.calc_layer_delta(downstream_layer)\n",
    "            downstream_layer = layer\n",
    "\n",
    "    def update_weight(self, rate):\n",
    "        \"\"\"\n",
    "        update weights of each connection or filter\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.update_weight(rate)\n",
    "    def predict(self, sample):\n",
    "        \"\"\"\n",
    "        predict output according to input\n",
    "        \"\"\"\n",
    "        self.layers[0].forward(sample)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].forward(self.layers[i-1].get_output())\n",
    "        return self.layers[-1].get_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, some interface method should be implemented in each layer class: *forward*, *calc_layer_delta*, *update_weight*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redesign of full connection layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to redesign the full connection layer class, for those reasons:\n",
    "- Full connect layers implemented with Node and Connection objects will encounter a great problem when docking with convolutional layer copied from hanbingtao.\n",
    "- Layers with large amount of nodes takes large memory storing their Node and Connection objects. Besides,  I'm afraid that efficiency problem will be caused when executing large-scale NN backpropagation algorithm with codes we have.\n",
    "\n",
    "To take advantage of numpy's matrix calculation ability, I will implement methods in fc layer using matrix operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FcLayer(object):\n",
    "    def __init__(self, network, node_count, activator):\n",
    "        self.output_array = np.zeros([node_count])\n",
    "        self.bias_array = np.zeros([node_count])\n",
    "        input_shape = network.layers[-1].get_output().shape\n",
    "        self.input_1dim = reduce(lambda ret, dim: ret * dim, input_shape, 1)\n",
    "        self.trans_matrix = np.random.uniform(0.1, 0.3, [node_count, self.input_1dim])\n",
    "        self.activator = activator\n",
    "        network.append_layer(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- *output_array*: output vector of a fc layer\n",
    "- *bias_array*: bias unit(or bias vector) of current layer\n",
    "- *input_1dim*: total number of the input(1D,2D,3D), useful for adapt convolutional layer's output to full connection layer's input\n",
    "- *trans_matrix*: transform matrix used to transform input array into output array\n",
    "- *activator*: activator function for output, ReLU was chosen in alexnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $W$ represent transform matrix, $\\vec x$ as input vector, $\\vec b$ as bias unit, $\\vec a$ as output vector, $f$ as activator function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "W=\\left[\n",
    "\\begin{matrix}\n",
    " w_{11}      & w_{12}      & \\cdots & w_{1m}      \\\\\n",
    " w_{21}      & w_{22}      & \\cdots & w_{2m}      \\\\\n",
    " \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " w_{n1}      & w_{n2}      & \\cdots & w_{nm}      \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    ",\\quad \\vec x=\\left[\n",
    "\\begin{matrix}\n",
    " x_{1}      \\\\\n",
    " x_{2}      \\\\\n",
    " \\vdots \\\\\n",
    " x_{m}      \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    ",\\quad \\vec b=\\left[\n",
    "\\begin{matrix}\n",
    " b_{1}      \\\\\n",
    " b_{2}      \\\\\n",
    " \\vdots \\\\\n",
    " b_{n}      \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    ",\\quad \\vec a=\\left[\n",
    "\\begin{matrix}\n",
    " a_{1}      \\\\\n",
    " a_{2}      \\\\\n",
    " \\vdots \\\\\n",
    " a_{n}      \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    ",\\quad f(\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    " x_{1}      \\\\\n",
    " x_{2}      \\\\\n",
    " \\vdots \\\\\n",
    " x_{n}      \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    ")=\\left[\n",
    "\\begin{matrix}\n",
    " f(x_{1})      \\\\\n",
    " f(x_{2})      \\\\\n",
    " \\vdots \\\\\n",
    " f(x_{n})      \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get equation like:\n",
    "$$\\vec a=f(W \\cdot \\vec x + \\vec b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the full layer's prediction procedure should be implemented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(self, input_array):\n",
    "        self.input_array = input_array.reshape(self.input_1dim)\n",
    "        self.output_array = np.dot(self.trans_matrix, self.input_array) + self.bias_array\n",
    "        self.output_array = np.array([self.activator.forward(value) for value in self.output_array])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The reason reshaping the input array is, it's neccessary to adapt input array to 1-D when accepting 3D-input from convolutional layer. \n",
    "\n",
    "There exist two cases to be considered when calculating $\\delta$ of each layer: $\\delta$ of output layer and non-output layer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\vec \\delta_{output}=\\vec y'*(\\vec t-\\vec y)$$\n",
    "$$\\vec \\delta_{non-output}=\\vec a'*(W_{downstream}^{T}\\cdot \\vec \\delta_{downstream})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_output_layer_delta(self, label):\n",
    "        derivative = np.array([self.activator.backward(out) for out in self.output_array])\n",
    "        self.delta_array = derivative * (label - self.output_array)\n",
    "\n",
    "def calc_layer_delta(self, downstream_layer):\n",
    "        derivative = np.array([self.activator.backward(out) for out in self.output_array])\n",
    "        self.delta_array = derivative * downstream_layer.get_transformed_delta()\n",
    "def get_transformed_delta(self):\n",
    "        return np.dot(self.trans_matrix.transpose(), self.delta_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next we can update transform matrix and bias vector of each layer with equations: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W\\leftarrow W + \\eta*\\vec \\delta \\cdot \\vec x^{T}$$\n",
    "$$\\vec b \\leftarrow \\vec b + \\eta * \\vec \\delta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_weight(self, rate):\n",
    "        self.trans_matrix += rate * np.dot(self.delta_array.reshape([len(self.delta_array),1]),\\\n",
    "                                           self.input_array.reshape([1, self.input_1dim]))\n",
    "        self.bias_array += rate * self.delta_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, my new full connection layer class has got the ability to finish a complete training procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Supplement of convolutional layer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With some little adjustment and supplement, code in conv_layer.py(copied from [[2]](https://www.zybuluo.com/hanbingtao/note/485480)) should dock well with full connection layer class above.\n",
    "\n",
    "First we need to implement a missed method called *get_patch*, which will return a specified slice from given 2D or 3D array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_patch(input_array, i, j, kernel_width, kernel_height, stride):\n",
    "    if len(input_array.shape) > 2:\n",
    "        return input_array[:, i*stride:i*stride+kernel_width, j*stride:j*stride+kernel_height]\n",
    "    return input_array[i*stride:i*stride+kernel_width, j*stride:j*stride+kernel_height]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of fitting with *network.py*, two interface method should be implemented(*forward* already prepared): *calc_layer_delta*, *update_weight*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_layer_delta(self, downstream_layer):\n",
    "    downstream_delta = downstream_layer.get_transformed_delta().reshape(self.output_array.shape)\n",
    "    self.bp_sensitivity_map(downstream_delta, self.activator)\n",
    "\n",
    "def update_weight(self, rate):\n",
    "    self.learning_rate = rate\n",
    "    self.bp_gradient(self.current_layer_delta_array)\n",
    "    self.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test run with FC and CONV layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we may have little cheers with classes by hand: start a test run with a network consisting of two convolutional layers and two full connection layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with one sample started\n",
      "   Procedure predict started\n",
      "      Forward calculation of layer 1 finished, time cost(s): 0.037877\n",
      "      Forward calculation of layer 2 finished, time cost(s): 0.004186\n",
      "      Forward calculation of layer 3 finished, time cost(s): 0.002823\n",
      "      Forward calculation of layer 4 finished, time cost(s): 2.7e-05\n",
      "   Procedure predict finished, time cost(s):0.045019\n",
      "   Procedure delta calculation started\n",
      "      Delta calculation of layer 4 finished, time cost(s): 1.9e-05\n",
      "      Delta calculation of layer 3 finished, time cost(s): 1.5e-05\n",
      "      Delta calculation of layer 2 finished, time cost(s): 0.560599\n",
      "      Delta calculation of layer 1 finished, time cost(s): 1.547325\n",
      "   Procedure delta calculation finished, time cost(s):2.108145\n",
      "   Procedure weight update started\n",
      "      Weight update of layer 1 finished, time cost(s): 0.121688\n",
      "      Weight update of layer 2 finished, time cost(s): 0.047791\n",
      "      Weight update of layer 3 finished, time cost(s): 0.000296\n",
      "      Weight update of layer 4 finished, time cost(s): 2.09999999998e-05\n",
      "   Procedure weight update finished, time cost(s):0.169944\n",
      "Training with one sample finished\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../python/\")\n",
    "from network import Network\n",
    "from fc_layer import FcLayer\n",
    "from conv_layer import ConvLayer\n",
    "from activator import ReluActivator as Relu\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fake_image = np.random.uniform(0,255,[3,50,50])\n",
    "    fake_label = np.zeros([10])\n",
    "    fake_label[5] = 1\n",
    "    net = Network()\n",
    "    ConvLayer(net, 50, 50, 3, 11, 11, 48, 2, 4, Relu(), 0.05)\n",
    "    ConvLayer(net, 11, 11, 48, 3, 3, 24, 2, 3, Relu(), 0.05)\n",
    "    FcLayer(net, 5, Relu())\n",
    "    FcLayer(net, 10, Relu())\n",
    "    net.train_one_sample(fake_label, fake_image, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructor improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, it seems a bit foolish to give the input size every time create a new convolutional layer--- we can get the input size from previous layer's output except first layer. We may create a class inherit from convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvLayerHidden(ConvLayer):\n",
    "    def __init__(self, network, filter_width,\n",
    "                 filter_height, filter_number,\n",
    "                 zero_padding, stride, activator,\n",
    "                 learning_rate):\n",
    "        upstream_layer = network.layers[-1]\n",
    "        input_width = upstream_layer.get_output().shape[1]\n",
    "        input_height = upstream_layer.get_output().shape[2]\n",
    "        channel_number = upstream_layer.get_output().shape[0]\n",
    "        ConvLayer.__init__(self, network, input_width, input_height,\n",
    "                           channel_number, filter_width,\n",
    "                           filter_height, filter_number,\n",
    "                           zero_padding, stride, activator,\n",
    "                           learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the procedure of creating network should act like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Network()\n",
    "    ConvLayer(net, 50, 50, 3, 11, 11, 48, 2, 4, Relu(), 0.05)\n",
    "    ConvLayerHidden(net, 3, 3, 24, 2, 3, Relu(), 0.05)\n",
    "    FcLayer(net, 5, Relu())\n",
    "    FcLayer(net, 10, Relu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with full-connection layer and convolutional layer, max-pooling layer is much more easier to be realized. Here I copied the code of max pooling layer from [[2]](https://www.zybuluo.com/hanbingtao/note/485480), then implement some interface methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def get_output(self):\n",
    "        return self.output_array\n",
    "\n",
    "    def get_transformed_delta(self):\n",
    "        return self.delta_array\n",
    "\n",
    "    def update_weight(self, rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some adjustment need to be made to method *backward*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def calc_layer_delta(self, downstream_layer):\n",
    "        self.delta_array = np.zeros(self.input_array.shape)\n",
    "        sensitivity_array = downstream_layer.get_transformed_delta().reshape(self.output_array.shape)\n",
    "        for d in range(self.channel_number):\n",
    "            for i in range(self.output_height):\n",
    "                for j in range(self.output_width):\n",
    "                    patch_array = get_patch(\n",
    "                        self.input_array[d], i, j,\n",
    "                        self.filter_width,\n",
    "                        self.filter_height,\n",
    "                        self.stride)\n",
    "                    k, l = get_max_index(patch_array)\n",
    "                    self.delta_array[d,\n",
    "                                     i * self.stride + k,\n",
    "                                     j * self.stride + l] = \\\n",
    "                        sensitivity_array[d, i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can dock fc-layer, conv-layer, and max-pooling-layer together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with one sample started\n",
      "   Procedure predict started\n",
      "      Forward calculation of layer 1 finished, time cost(s): 0.044907\n",
      "      Forward calculation of layer 2 finished, time cost(s): 0.003826\n",
      "      Forward calculation of layer 3 finished, time cost(s): 0.001792\n",
      "      Forward calculation of layer 4 finished, time cost(s): 0.000289\n",
      "      Forward calculation of layer 5 finished, time cost(s): 5.50000000001e-05\n",
      "      Forward calculation of layer 6 finished, time cost(s): 1.80000000003e-05\n",
      "   Procedure predict finished, time cost(s):0.051029\n",
      "   Procedure delta calculation started\n",
      "      Delta calculation of layer 6 finished, time cost(s): 1.49999999999e-05\n",
      "      Delta calculation of layer 5 finished, time cost(s): 1.3e-05\n",
      "      Delta calculation of layer 4 finished, time cost(s): 0.000381\n",
      "      Delta calculation of layer 3 finished, time cost(s): 0.123552\n",
      "      Delta calculation of layer 2 finished, time cost(s): 0.005156\n",
      "      Delta calculation of layer 1 finished, time cost(s): 1.513446\n",
      "   Procedure delta calculation finished, time cost(s):1.642747\n",
      "   Procedure weight update started\n",
      "      Weight update of layer 1 finished, time cost(s): 0.132405\n",
      "      Weight update of layer 2 finished, time cost(s): 4.00000000056e-06\n",
      "      Weight update of layer 3 finished, time cost(s): 0.050049\n",
      "      Weight update of layer 4 finished, time cost(s): 1.00000000014e-06\n",
      "      Weight update of layer 5 finished, time cost(s): 3.50000000005e-05\n",
      "      Weight update of layer 6 finished, time cost(s): 9.00000000037e-06\n",
      "   Procedure weight update finished, time cost(s):0.182846\n",
      "Training with one sample finished\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../python/\")\n",
    "from network import Network\n",
    "from fc_layer import FcLayer\n",
    "from conv_layer import ConvLayer\n",
    "from conv_layer_hidden import ConvLayerHidden\n",
    "from activator import ReluActivator as Relu\n",
    "from max_pooling_layer import MaxPoolingLayer\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fake_image = np.random.uniform(0, 255, [3, 50, 50])\n",
    "    fake_label = np.zeros([10])\n",
    "    fake_label[5] = 1\n",
    "    net = Network()\n",
    "    ConvLayer(net, 50, 50, 3, 11, 11, 48, 2, 4, Relu(), 0.05)\n",
    "    MaxPoolingLayer(net, 3, 3, 2)\n",
    "    ConvLayerHidden(net, 3, 3, 24, 2, 3, Relu(), 0.05)\n",
    "    MaxPoolingLayer(net, 2, 2, 1)\n",
    "    FcLayer(net, 5, Relu())\n",
    "    FcLayer(net, 10, Relu())\n",
    "    net.train_one_sample(fake_label, fake_image, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance optimization"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
